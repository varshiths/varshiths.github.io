<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Varshith Sreeramdass</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://varshiths.github.io" />
	    <meta property="og:title" content="Varshith Sreeramdass" />
	    <meta property="og:image" content="https://varshiths.github.io/img/me_2023.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Varshith Sreeramdass">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
    </head>
    <body>
        <div class="container mt-5">
            <div class="row mb-3">
                <div class="col">
                    <h1>Varshith Sreeramdass</h1>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1">
                    <div class="card mb-3">
                        <img class="card-img-top" src="img/me_2023.jpg" alt="Varshith Sreeramdass">
                        <div class="card-body">
                            <h5 class="card-title">
                                <b>Varshith Sreeramdass</b>
                            </h5>
                            <p class="card-text">
                                Robotics PhD
                                </br>
                                Georgia Institute of Technology
                                </br>
                                Atlanta, US
                                </br>
                                </br>
                                Email: vsreeramdass@gatech.edu
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
                    <p>
                        Hi! I'm a PhD student at <a href="https://www.gatech.edu/">Georgia Institute of Technology</a>, advised by Prof. <a href="https://core-robotics.gatech.edu/">Matthew Gombolay</a> and Prof. <a href="https://www.epic.gatech.edu/">Aaron Young</a>. My research lies in robot learning and human-robot interaction. I am honoured to be supported by the Robotics PhD fellowship from the <a href="https://research.gatech.edu/robotics">Institute of Robotics and Intelligent Machines</a>.
                    </p>
                    <p>
                        I did my MS in Computer Science at <a href="https://www.gatech.edu/">Georgia Tech</a>, specializing in Computational Perception and Robotics. Before that, I was a Research Engineer at <a href="https://www.honda.co.jp/robotics" target="_blank">Honda R&D Robotics</a>. My undergraduate studies were at <a href="https://www.iitb.ac.in" target="_blank">IIT Bombay</a> in Computer Science and Engineering, where I was advised by Prof. <a href="https://www.cse.iitb.ac.in/~sunita/">Sunita Sarawagi</a> and Prof. <a href="https://www.cse.iitb.ac.in/~soumen/">Soumen Chakrabarti</a>. I also interned at <a href="https://www.jp.honda-ri.com/en/">Honda Research Institute Japan</a>.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p>
                        Links:
                        [<a href="res/CV.pdf" target="_blank">CV</a>] [<a href="https://scholar.google.com/citations?user=9gGBTZEAAAAJ&hl=en" target="_blank">Google Scholar</a>] [<a href="https://github.com/varshiths" target="_blank">Github</a>] [<a href="https://www.linkedin.com/in/vsreeramdass/" target="_blank">LinkedIn</a>]
                    </p>
                </div>
            </div>
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Projects and Publications</h2>
                    <ul class="pl">
                        <li>
                            <b>Generalized Behavior Learning from Diverse Demonstrations</b>
                            <br/>
                            <b>Varshith Sreeramdass</b>,
                            Rohan Paleja, Letian Chen, Sanne van Waveren and
                            Matthew Gombolay.
                            <br/>
                            <b>
                                First workshop on Out-of-Distribution Generalization in Robotics at CoRL 2023 (Oral).
                            </b>
                            <br/>
                            Georgia Institute of Technology, 2023.
                            <br/>
                            [<a href="#" onclick="$('#gatech_generalized').toggle();return false;">Abstract</a>]
                            [<a href="https://openreview.net/forum?id=5uEkcZZCnk" target="_blank">Paper</a>]
                            [<a href="https://github.com/CORE-Robotics-Lab/GSD" target="_blank">Code</a>]
                            [<a href="https://varshiths.github.io/res/gsd_corlw_post.pdf" target="_blank">Poster</a>]
                            <div id="gatech_generalized" class="abstract" style="display:true;">
                                <div class="row align-items-center">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/gsdw.png" alt="gsdw">
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                            Learning robot control policies through reinforcement learning can be challenging due to the complexity of designing rewards, which often result in unexpected behaviors. Imitation Learning overcomes this issue by using demonstrations to create policies that mimic expert behaviors. However, experts often demonstrate varied approaches to tasks. Capturing this variability is crucial for understanding and adapting to diverse scenarios. Prior methods capture variability by optimizing for behavior diversity alongside imitation. Yet, naive formulations of diversity can result in meaningless representation of latent factors, hindering generalization to novel scenarios. We propose Guided Strategy Discovery (GSD), a novel regularization method that specifically promotes expert-specified, task-relevant diversity. In the recovery of unseen expert behaviors, GSD improves 11% over the next best baseline across three continuous control tasks on average.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Structured Policies in Reinforcement Learning for Dexterous Manipulation</b>
                            <br/>
                            <b>Varshith Sreeramdass</b>,
                            Akinobu Hayashi, Tadaaki Hasegawa
                            and Takayuki Osa.
                            <br/>
                            Honda R&D Robotics, 2022.
                            <br/>
                            [<a href="#" onclick="$('#honda_structured').toggle();return false;">Abstract</a>]
                            [<a href="https://www.linkedin.com/posts/open-source-robotics-foundation_iros2022-ros-ugcPost-6990934827885625344-Rwto/" target="_blank">Demo, IROS '22</a>]
                            <div id="honda_structured" class="abstract" style="display:true;">
                                <div class="row align-items-center">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/canhand.png" alt="proto2_rl">
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                            For real world manipulation through reinforcement learning, exploration presents a significant challenge. Policies with incorporated structure can improve exploration efficiency, making learning feasible. This project explores structure in two forms: (1) parameterized motion primitives as low level policies, and gating policies operating in goal and duration spaces, and (2) residual policies that provide adjustments to actions from scripted base controllers. While simulation results showed that parameterized primitives accelerated learning, performance was limited in scenarios requiring quick, reactive behaviors. In comparison, residual policies showed significant improvements in sample efficiency and robustness to noisy initial conditions. Residual policies transferred through sim-to-real proved robust to an object initialization noise of 3mm in a beverage can-opening task requiring precise contacts and high torques.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Data-driven DRL for Dexterous InHand Manipulation</b>
                            <br/>
                            <b>Varshith Sreeramdass</b>,
                            Akinobu Hayashi and
                            Tadaaki Hasegawa.
                            <br/>
                            Honda R&D Robotics, 2021.
                            <br/>
                            [<a href="#" onclick="$('#honda_datarl').toggle();return false;">Abstract</a>]
                            [<a href="https://global-honda.translate.goog/jp/tech/Avatar_robot/?_x_tr_sl=ja&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp" target="_blank">Press Release</a>]
                            [<a href="https://www.youtube.com/watch?v=tS9LL2oVbYI" target="_blank">Video</a>]
                            <div id="honda_datarl" class="abstract" style="display:true;">
                                <div class="row align-items-center">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/proto2_rl.jpg" alt="proto2_rl">
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                        Learning dexterous manipulation directly on real robots is considerably difficult to do from scratch due to extensive real robot deployment. Sim-to-real is not straight forward due to simulation fidelity.
                                        In this work, we employed data-driven reinforcement learning algorithms (DAPG, AWAC) to learn from expert and suboptimal demonstrations. In tasks involving reorientation of objects for tool-use (transitions among precision, tripod and power grasps), we achieved robustness towards noise in object initialization.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Domain Adaptation of Cloud NLP Services through Word Substitutions</b>
                            <br/>
                            <b>Varshith Sreeramdass</b>,
                            Vihari Piratla,
                            Sunita Sarawagi and
                            Soumen Chakrabarti.
                            <br/>
                            <b>B. Tech. Thesis (Part II)</b>, IIT Bombay, 2019.
                            <br/>
                            [<a href="#" onclick="$('#btp_2_cloud_nlp_abstract').toggle();return false;">Abstract</a>]
                            [<a href="res/BTP_II.pdf" target="_blank">Report</a>]
                            [<a href="res/BTP_II_Survey.pdf" target="_blank">Survey</a>]
                            <div id="btp_2_cloud_nlp_abstract" class="abstract" style="display:true;">
                                <div class="row">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/cloud_rephrase.png" alt="cloud_rephrase">
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                        Several cloud services are available which perform natural language tasks 
                                        like sentiment classification, named entity recognition, dependency parsing, fine type tagging, etc. 
                                        While these services are trained on a large diverse dataset 
                                        encompassing a large number of domains, the performance of the service 
                                        on a specific narrow domain relevant to the application may be sub par. 
                                        For the tasks of sentiment classification and NER, 
                                        this work attempts to adapt sentences in the target domain 
                                        to the domain of the service model, to improve 
                                        the performance of the service model, through word substitutions.
                                        </p>
                                        <p>
                                        Additionally, this project also surveyed in depth, methods to learn an optimal active learner using reinforcement learning.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Out-of-Distribution Image Detection with Deep Neural Networks</b>
                            <br/>
                            <b>Varshith Sreeramdass</b> and
                            Sunita Sarawagi.
                            <br/>
                            <b>B. Tech. Thesis (Part I)</b>, IIT Bombay, 2019.
                            <br/>
                            [<a href="#" onclick="$('#btp_1_ood').toggle();return false;">Abstract</a>]
                            [<a href="res/BTP_I.pdf" target="_blank">Report</a>]
                            <div id="btp_1_ood" class="abstract" style="display:true;">
                                <div class="row">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/odin.png" alt="ood">
                                        <p>Image: <a href="https://arxiv.org/pdf/1706.02690.pdf">Liang, et. al., 2017</a></p>
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                        Out-of-distribution detection is vital in the deployment of Deep Neural Networks
                                        in practical applications. Several methods exist that range from temperature scaling
                                        of logits to model ensembles.  This work surveys extensively existing
                                        methods for out-of-distribution detection that either use features extracted by
                                        pre-trained models, or propose minimal modifications to the framework, analyses
                                        their performance, shortcomings and proposes a few simple enhancements that produce comparable results.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Augmenting Scene Graph Generation with Knowledge from Corpora</b>
                            <br/>
                            <b>Varshith Sreeramdass</b>,
                            Amrita Saha and
                            Soumen Chakrabarti.
                            <br/>
                            Independent Study Under Faculty, IIT Bombay, Spring 2019.
                            <br/>
                            [<a href="#" onclick="$('#r_and_d_proj_augmenting').toggle();return false;">Abstract</a>]
                            [<a href="https://github.com/varshiths/scg-augmented/blob/master/REPORT.md" target="_blank">Report</a>]
                            <div id="r_and_d_proj_augmenting" class="abstract" style="display:true;">
                                <div class="row">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/lk_distill.png" alt="lk_distill">
                                        <p>Image: <a href="https://arxiv.org/abs/1707.09423">Yu, et al., 2017</a></p>    
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                        Scene graph generation is vital to structuring the information in a visual scene. 
                                        It is also one of the most challenging tasks in computer vision, requiring an enormous amount of data to learn from. 
                                        The task can thus benefit from side information, knowledge that is not necessarily in the form of image annotations, 
                                        but a distribution over the edges of a scene graph obtained from the likes of a relevant knowledge base or a text corpus.
                                        This work attempts to study the method of Linguistic Knowledge Distillation (Yu, Ruichi, et al., 2017), and enhance scene graph generation 
                                        by augmenting with information from various text corpus describing visual scenes, objects and their spatial relationships.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Sign Language Synthesis with Styling</b>
                            <br/>
                            <b>Varshith Sreeramdass</b> and
                            Heike Brock.
                            <br/>
                            Internship, HRI-JP, Summer 2018.
                            <br/>
                            [<a href="#" onclick="$('#intern_hrijp_sls').toggle();return false;">Abstract</a>]
                            [<a href="res/HRIJP.pdf" target="_blank">Report</a>]
                            [<a href="https://github.com/varshiths/pr-scgan" target="_blank">Code</a>]
                            <div id="intern_hrijp_sls" class="abstract" style="display:true;">
                                <div class="row">
                                    <div class="col-md-3 text-center my-auto">
                                        <img src="img/jsl_char.png" alt="jsl_char">
                                    </div>
                                    <div class="col-md-9">
                                        <p>
                                        To enable communication between those that are ignorant and fluent in sign langauge,
                                        seq-to-seq models that generate joint trajectories of virtual characters from sign-language tokens can be leveraged.
                                        However, such generated motion tends to feel robotic and lacks natural-human variability.
                                        The work attempts to build upon such models to enable variation in style.
                                        The model is coupled with style parameters that
                                        are learnt in an unsupervised manner using a GAN approach.
                                        While the model fails in generating reliable motion usable for 
                                        communication, the work does explore a representations for motion capture
                                        data and invalidate using certain types.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <br/>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <h2>Miscellaneous</h2>
                    <ul>
                        <li>
                            In my free time, I like to try new food, listen to rock music, visit art museums and (occasionally) hike.
                        </li>
                        <li>
                            I speak English, the Rayalaseema dialect of Telugu, Hindi and N3 Japanese.
                        </li>
                        <li>
                            I borrowed the template for this website from <a href="https://nelsonliu.me/"
                            target="_blank">Nelson Liu</a>'s homepage.
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            Last updated in May 2024.
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.iitb.ac.in" class="image-link">
                            <img class="mr-4" src="img/iitb.png" alt="IITB logo." height="75">
                        </a>
                        <a href="https://www.honda.co.jp/robotics" class="image-link">
                            <img src="img/honda.png" alt="Honda logo." height="75">
                        </a>
                        <a href="https://ic.gatech.edu/" class="image-link">
                            <img src="img/gt.png" alt="GaTech logo." height="75">
                        </a>
                    </div>
                </div>
            </footer>
        </div>
    </body>
</html>
